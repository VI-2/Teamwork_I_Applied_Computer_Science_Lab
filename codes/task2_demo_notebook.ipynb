{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51660c7-ac9a-4225-baf8-6211ed965732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.3.5\n",
      "Uninstalling numpy-2.3.5:\n",
      "  Successfully uninstalled numpy-2.3.5\n",
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d3af88-5cda-4199-a8e2-950cb52b8c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.4.22 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (0.4.22)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (2.32.5)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (2.12.5)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.124.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (2.3.5)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (4.15.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (3.8.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.39.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (9.1.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from chromadb==0.4.22) (5.2.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.4.22) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.4.22) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.4.22) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.4.22) (0.0.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.4.22) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.4.22) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.4.22) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22) (3.11)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2025.11.12)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (6.33.2)\n",
      "Requirement already satisfied: sympy in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.22) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.22) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb==0.4.22) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.60b0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.60b0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.60b0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.60b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (1.17.3)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (3.11.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb==0.4.22) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb==0.4.22) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from requests>=2.28->chromadb==0.4.22) (3.4.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb==0.4.22) (0.36.0)\n",
      "Requirement already satisfied: filelock in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.4.22) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.4.22) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.4.22) (14.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22) (0.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.16.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.22) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.4.22) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mariam/Desktop/rag_env_py312/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.22) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.4.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd2b649-6073-4971-a90a-87d9850c38d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "print(\"✓ ChromaDB imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cd13c",
   "metadata": {},
   "source": [
    "# Demo Notebook: Embeddings & Vector Search (Task 2)\n",
    "This notebook demonstrates dataset loading, chunking, embedding, storing, and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a11ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead254c",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92f1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Dataset loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_text = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources.\n",
    "Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
    "RAG solves this by connecting retrieval systems with generative models.\n",
    "First, relevant documents are retrieved using similarity search techniques.\n",
    "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing.\n",
    "This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
    "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation.\n",
    "With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
    "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline.\n",
    "\n",
    "When writing queries for RAG systems, it is important to:\n",
    "- Be clear and concise\n",
    "- Use domain-specific keywords\n",
    "- Include context when possible\n",
    "- Avoid vague pronouns\n",
    "Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
    "\n",
    "The pipeline often involve:\n",
    "1. Preprocessing datasets\n",
    "2. Chunking text into meaningful pieces\n",
    "3. Creating embeddings for each chunk\n",
    "4. Storing embeddings in a vector database\n",
    "5. Computing similarity between query and chunks\n",
    "6. Retrieving top-K chunks for LLM input\n",
    "\n",
    "RAG reduces hallucination by grounding LLM responses in retrieved context.\n",
    "Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Step 1: Dataset loaded successfully.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 1 failed:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770c83a",
   "metadata": {},
   "source": [
    "##Step 2: Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a18f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Created 13 meaningful chunks.\n",
      "\n",
      "Chunk 1:\n",
      "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources. Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
      "\n",
      "Chunk 2:\n",
      "RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques.\n",
      "\n",
      "Chunk 3:\n",
      "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing. This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
      "\n",
      "Chunk 4:\n",
      "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation. With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
      "\n",
      "Chunk 5:\n",
      "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
      "\n",
      "Chunk 6:\n",
      "The pipeline often involve:\n",
      "\n",
      "Chunk 7:\n",
      "1. Preprocessing datasets\n",
      "\n",
      "Chunk 8:\n",
      "2. Chunking text into meaningful pieces\n",
      "\n",
      "Chunk 9:\n",
      "3. Creating embeddings for each chunk\n",
      "\n",
      "Chunk 10:\n",
      "4. Storing embeddings in a vector database\n",
      "\n",
      "Chunk 11:\n",
      "5. Computing similarity between query and chunks\n",
      "\n",
      "Chunk 12:\n",
      "6. Retrieving top-K chunks for LLM input\n",
      "\n",
      "Chunk 13:\n",
      "RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Split into lines, remove empty\n",
    "    lines = [line.strip() for line in dataset_text.splitlines() if line.strip()]\n",
    "    \n",
    "    # Merge 2–3 sentences per chunk, keep numbered lists intact\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        # Handle numbered list items as separate chunks\n",
    "        if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, 20))):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            chunks.append(line.strip())\n",
    "        else:\n",
    "            # Split line into sentences\n",
    "            sentences = re.split(r'(?<=[.!?]) +', line)\n",
    "            for s in sentences:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \" \" + s\n",
    "                else:\n",
    "                    current_chunk = s\n",
    "                if current_chunk.count('.') >= 2:  # 2 sentences per chunk\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "    # Append leftover\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    print(f\"Step 2: Created {len(chunks)} meaningful chunks.\\n\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{c}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 2 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f394b",
   "metadata": {},
   "source": [
    "## Step 3: Generate embeddings (requires sentence-transformers package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Sentence Transformer model loaded.\n",
      "\n",
      "Step 3: Embeddings generated for 13 chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"Step 3: Sentence Transformer model loaded.\\n\")\n",
    "    \n",
    "    chunk_embeddings = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f\"Step 3: Embeddings generated for {len(chunks)} chunks.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 3 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ed141",
   "metadata": {},
   "source": [
    "## Step 3:FIASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0da74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: FAISS index created and 13 embeddings added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dimension = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  \n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Step 4: FAISS index created and {len(chunks)} embeddings added.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 4 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30566d3",
   "metadata": {},
   "source": [
    "## Step 5:Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0110faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Query transformed into embedding: 'How does RAG reduce hallucination?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = \"How does RAG reduce hallucination?\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f\"Step 5: Query transformed into embedding: '{query}'\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 5 failed:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe59590",
   "metadata": {},
   "source": [
    "## Step 6: Text retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b08b72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does RAG reduce hallucination?\n",
      "\n",
      "Retrieved Top-4 Chunks:\n",
      "\n",
      "Result 1 (score=0.529):\n",
      "RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
      "\n",
      "Result 2 (score=0.264):\n",
      "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
      "\n",
      "Result 3 (score=0.224):\n",
      "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources. Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
      "\n",
      "Result 4 (score=0.216):\n",
      "RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    top_k = 5\n",
    "    threshold = 0.1  # Minimum cosine similarity\n",
    "\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    retrieved_chunks = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score >= threshold:\n",
    "            retrieved_chunks.append((score, chunks[idx]))\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        print(f\"Query: {query}\\n\\nNo relevant chunks found for this query.\\n\")\n",
    "    else:\n",
    "        print(f\"Query: {query}\\n\\nRetrieved Top-{len(retrieved_chunks)} Chunks:\\n\")\n",
    "        for i, (score, chunk) in enumerate(retrieved_chunks):\n",
    "            print(f\"Result {i+1} (score={score:.3f}):\\n{chunk}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 6 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "352f7bf9-9888-4d3a-862e-014cb14b262b",
   "metadata": {},
   "source": [
    "VECTOR DATABASE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c06153-545b-4778-99df-180d146887e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, collection_name=\"rag_documents\"):\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        # Delete collection if exists (for clean testing)\n",
    "        try:\n",
    "            self.client.delete_collection(name=collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.collection = self.client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(f\"✓ VectorDB initialized with ChromaDB collection: {collection_name}\")\n",
    "\n",
    "    def add_docs(self, chunks, metadata=None):\n",
    "        if not metadata:\n",
    "            metadata = [{'source': 'unknown', 'chunk_id': i} for i in range(len(chunks))]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.model.encode(chunks, convert_to_numpy=True)\n",
    "        ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "        \n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=chunks,\n",
    "            metadatas=metadata,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Added {len(chunks)} documents to ChromaDB\")\n",
    "        return True\n",
    "\n",
    "    def search(self, query, top_k=5):\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            formatted_results.append({\n",
    "                'chunk': results['documents'][0][i],\n",
    "                'score': 1 - results['distances'][0][i],\n",
    "                'metadata': results['metadatas'][0][i]\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'total_chunks': self.collection.count(),\n",
    "            'model': 'all-MiniLM-L6-v2',\n",
    "            'database': 'ChromaDB'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b94dd8-ae90-48f0-b544-a82c7d1ba8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ VectorDB initialized with ChromaDB collection: rag_documents\n",
      "✓ Added 13 documents to ChromaDB\n",
      "{'chunk': 'Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.', 'score': 0.44305121898651123, 'metadata': {'page': 4, 'source': 'test.txt'}}\n"
     ]
    }
   ],
   "source": [
    "db = VectorDB()\n",
    "db.add_docs(chunks, metadata=[{'source': 'test.txt', 'page': i} for i in range(len(chunks))])\n",
    "results = db.search(\"How does RAG work?\")\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427101e-00ba-42aa-a67e-612f03582738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (RAG)",
   "language": "python",
   "name": "rag_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
